{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L1_Tokenizer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manjunath727/DLwithTF/blob/master/NLP_Using_TF/W1/L1_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKtADMNhG543",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "    Tokenization is the process of converting text into numeric values with a number representing a word or character. \n",
        "     \n",
        "    Keras provides a tokenizer class for generating dictionary of word encodings and creating vector out of sentences. Tokenizer provides a word index property which returns a dictionary containing key value pairs, where key is the word and value is the token for the word. \n",
        "    \n",
        "    Tokenizer strips the punctuations out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axuIxclOJStB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57e9513f-2d5e-424b-a583-419504a228b5"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'Chelsea beat Barcelona',\n",
        "    'I pity Arsenal',\n",
        "    'Mou is the best manager in the world'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_indices = tokenizer.word_index\n",
        "print(word_indices)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'chelsea': 2, 'beat': 3, 'barcelona': 4, 'i': 5, 'pity': 6, 'arsenal': 7, 'mou': 8, 'is': 9, 'best': 10, 'manager': 11, 'in': 12, 'world': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYB_pIQBMWjI",
        "colab_type": "text"
      },
      "source": [
        "## Text to Sequences\n",
        "\n",
        "    Next is turning sentences into a list of values using the tokens generated above.\n",
        "    Tokenizer provides texts_to_sequences method. \n",
        "    texts_to_sequences takes any sets of sentences and encodes them based on the word-set that the tokenizer learnt using fit_on_texts()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT2bfdTjKHAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e35e5fd-8541-4fc5-c3bb-8e1602a6bf5a"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 4], [5, 6, 7], [8, 9, 1, 10, 11, 12, 1, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny3NrSRUPABf",
        "colab_type": "text"
      },
      "source": [
        "## Testing \n",
        "\n",
        "    Now testing this tokenizer on new sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztZxj8N5OsXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d52cbbe3-5c6a-4b83-d233-fd857f266f54"
      },
      "source": [
        "# Try with words that tokenizer was not fit\n",
        "\n",
        "test_sentences = [\n",
        "    'Mou is the greatest football manager in the world',\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "print(test_seq)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8, 9, 1, 11, 12, 1, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED9WJiKpSpP4",
        "colab_type": "text"
      },
      "source": [
        "## Out-of-vocabulary and Padding\n",
        "\n",
        "    In the above test-data, there were words like 'greatest', 'football', which the tokenizer was not earlier fit on. Hence when tokenizer.texts_to_sequences was run on such sentences, those words were not encoded. To avoid this scenario, declare tokenizer with a property oov_token\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUSuFwzrPyCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tokenizer = Tokenizer(num_words=100,oov_token='<OOV>')\n",
        "new_tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "sequences = new_tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC7FOd_TUgfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26cb03b8-44a5-4ae7-f60a-d52cfaf8252a"
      },
      "source": [
        "test_seq = new_tokenizer.texts_to_sequences(test_sentences)\n",
        "print(test_seq)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9, 10, 2, 1, 1, 12, 13, 2, 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bKaUaCsVHLg",
        "colab_type": "text"
      },
      "source": [
        "### Padding\n",
        "\n",
        "    As with images which were required to be of uniform size when passing into a neural network, similar uniformity needs to be maintained for texts which are sent to train in an neural net.\n",
        "    \n",
        "    Pad sequences tool from keras allows sentences of different length to either truncate or pad them to make all sentences of same length. \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70qgafEbUzli",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "afef3565-ba0a-47ea-83e1-fb76ed40bb4a"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_sequences = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "print(sequences)\n",
        "print(padded_sequences)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 4, 5], [6, 7, 8], [9, 10, 2, 11, 12, 13, 2, 14]]\n",
            "[[ 0  0  3  4  5]\n",
            " [ 0  0  6  7  8]\n",
            " [11 12 13  2 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q00WjlQjW70R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}