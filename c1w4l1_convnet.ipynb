{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled27.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manjunath727/DLwithTF/blob/master/c1w4l1_convnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b16u0dn3z5Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Horse or Human \n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMVJXjvr96Mj",
        "colab_type": "text"
      },
      "source": [
        "Pre stage : Involves Image download, Visualizing images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oADbQT3A0HBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Access the zip file and unzip the data\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH9kAMsl0kD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBI9AnEl4RUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the filenames in horses and human directories\n",
        "train_horse_names = os.listdir('/tmp/horse-or-human/horses/')\n",
        "#print(train_horse_names[:10])\n",
        "\n",
        "train_human_names = os.listdir('/tmp/horse-or-human/humans')\n",
        "#print(train_human_names[:10])\n",
        "\n",
        "# Check the length of image\n",
        "print(len(os.listdir('/tmp/horse-or-human/horses')))\n",
        "print(len(os.listdir('/tmp/horse-or-human/humans')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRzazCBj5tIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a look at some of the pictures\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph. We'll output images in 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_CZTOL70bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display batch of 8 horses and 8 human pictures\n",
        "\n",
        "# Setup matplotlib fig and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname)\n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "\n",
        "next_human_pix = [os.path.join(train_human_dir, fname)\n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVRR1aDD-a7A",
        "colab_type": "text"
      },
      "source": [
        "**Building a model from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B26tImjz-rlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Build a convnet model just like before. Because it is a binary classification\n",
        "# We use Sigmoid function at the \n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Desired input shape (300x300x3) 3 bytes with color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16,(3,3), activation='relu', input_shape=(300,300,3)),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    # Second convolution\n",
        "    tf.keras.layers.Conv2D(32,(3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TdPSCkx_YzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuCkQ06B7zX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onJdkiqEC7AS",
        "colab_type": "text"
      },
      "source": [
        "**Data Processing**\n",
        "Set up data generators that read pictures, convert them to float tensors and feed them to network. We'll have one generator for training images, and one for validation images. Generators will yield batches of images of size 300 x 300 and their labels\n",
        "\n",
        "Pre process pixel image by normalzing pixel values between [0,1]\n",
        "\n",
        "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class using the rescale parameter. This ImageDataGenerator class allows you to instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs: fit_generator, evaluate_generator, and predict_generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpeJA6BcCjJN",
        "colab_type": "code",
        "outputId": "334f5cae-49b0-4ff6-f3a7-250883032c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/tmp/horse-or-human/',\n",
        "    target_size=(300,300),\n",
        "    batch_size=128,\n",
        "    # Since we use binary_crossentropy loss, we need binary labels\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLao1ssCB-Ei",
        "colab_type": "text"
      },
      "source": [
        "Train the model. \n",
        "\n",
        "Train for 15 epochs. The loss and accuracy are indication of progress of training. It's making a guess as to the classification of data and measuring it against the known label, calculating the result. Accuracy is a portion of correct guesses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyARCF74E-4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = 8,\n",
        "    epochs = 15,\n",
        "    verbose = 1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxY1cRwsFSPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN0LSOEIu8Ex",
        "colab_type": "text"
      },
      "source": [
        "# Running the model\n",
        "\n",
        "The following code picks file from the filesystem and runs it through the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1NX-aXbycdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2f8lMY-wN-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300,300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  \n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0] > 0.5:\n",
        "    print(fn + ' is a human')\n",
        "  else:\n",
        "    print(fn + ' is a horse')   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUJ7TSEvIGw8",
        "colab_type": "text"
      },
      "source": [
        "**Visualizing through the convnet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_JtfBsxIPX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new model that will take and image\n",
        "# as an input. And output intermediate representations\n",
        "# for all layers in previous model after first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "# Visualization model = Model(img_input, successive_inputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random image from the training set\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300,300))\n",
        "x = img_to_array(img)\n",
        "x = x.reshape((1,) + x.shape)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations of this image\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the name ofthe layers, so can have them as a part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Now lets display our representaions\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for conv/maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1] #number of features in feature map\n",
        "    # the feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0,:,:,i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # Tile each filter into big horizontal grid\n",
        "      display_grid[:, i*size : (i+1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20./n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_kxPVpbJ8Vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}