{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manjunath727/DLwithTF/blob/master/CNN_Using_TF/W2/ConvNet_with_DataAugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11P2cykWVA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy0TCdCpWcV2",
        "colab_type": "text"
      },
      "source": [
        "## Horse or Human with Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqbz0UduWyCz",
        "colab_type": "text"
      },
      "source": [
        "### Download data, setup dirs\n",
        "\n",
        "Following code download the zip file, extracts and sets up training and validation directories for horses and humans\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27h-a2I0WxMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "feed454c-109d-4d9a-89be-eb20b3e54994"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip\n",
        "  \n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "with zipfile.ZipFile(local_zip) as zip_ref:\n",
        "  zip_ref.extractall('/tmp/horse-or-human')\n",
        "  zip_ref.close()\n",
        "\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "with zipfile.ZipFile(local_zip) as zip_ref:\n",
        "  zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "  zip_ref.close()\n",
        "\n",
        "# Directory with training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses/')\n",
        "\n",
        "# Directory with training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with validation horse pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses/')\n",
        "\n",
        "# Directory with validation human pictures\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-06 14:04:09--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c03::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘/tmp/horse-or-human.zip’\n",
            "\n",
            "/tmp/horse-or-human 100%[===================>] 142.65M  94.5MB/s    in 1.5s    \n",
            "\n",
            "2019-07-06 14:04:11 (94.5 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n",
            "--2019-07-06 14:04:12--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 2607:f8b0:400e:c03::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘/tmp/validation-horse-or-human.zip’\n",
            "\n",
            "/tmp/validation-hor 100%[===================>]  10.95M  48.7MB/s    in 0.2s    \n",
            "\n",
            "2019-07-06 14:04:13 (48.7 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpCfCDoCbyMq",
        "colab_type": "text"
      },
      "source": [
        "### Build a model\n",
        "First we define our neural network model. As usual we use convolutional layers and flatten the final result to feed it to a dense network layer. Finally we add the dense connected layers.\n",
        "\n",
        "Since this is a binary classification problem, we use sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-GbKmgJX1Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUUi1hWgfD7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "e64fd40b-6b2b-42de-fbb7-a4f6714a9694"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 color\n",
        "    # bytes\n",
        "    # First convolution\n",
        "    tf.keras.layers.Conv2D(16,(3,3), activation='relu', input_shape=(300,300,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Second convolution\n",
        "    tf.keras.layers.Conv2D(32,(3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Third convolution\n",
        "    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Fourth convolution\n",
        "    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Fifth convolution\n",
        "    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron.\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0706 14:04:16.642902 140114716776320 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFmKAZZriHmc",
        "colab_type": "text"
      },
      "source": [
        "### **Compile the model**\n",
        "\n",
        "We compile the model with loss functin set as 'binary_cross_entropy' and use RMSprop as the optimizer to set the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMm4XSmeho2A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "cff19900-c765-4c05-9671-f1627d32a4c7"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0706 14:04:16.999677 140114716776320 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzgVTuIEkojT",
        "colab_type": "text"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "`ImageDataGenerator` class of keras library provides methods to generate batches tensor data with real time data augmentation. The data will be looped over in batches.\n",
        "These generators can then be used with Keras model methods that accept data generators as inputs: `fit_generator`, `evaluate_generator` and `predict_generator`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W313MGwbkN5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk5SexL4qEF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "05d1109d-191b-48bf-f462-54ea3ce232e5"
      },
      "source": [
        "# all images will be scaled by 1./255.\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/tmp/horse-or-human',       # Source dir of the training images\n",
        "    target_size=(300,300),\n",
        "    batch_size=128,\n",
        "    # Since we use binary cross_entropy loss, we need nbinary labels\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Flow training images in batches of 32 using validataion datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    '/tmp/validation-horse-or-human', # Source dir of the validation images\n",
        "    target_size=(300,300),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary'\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE0PQVvftjD_",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuXII4TYtThf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "d0283098-3a5f-44df-891f-a06edde8421f"
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=8,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps=8\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 107s 13s/step - loss: 0.6883 - acc: 0.5261 - val_loss: 0.6975 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 103s 13s/step - loss: 0.6734 - acc: 0.5662 - val_loss: 0.6613 - val_acc: 0.5039\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 103s 13s/step - loss: 0.6528 - acc: 0.5706 - val_loss: 0.6163 - val_acc: 0.8008\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 103s 13s/step - loss: 0.6451 - acc: 0.6730 - val_loss: 0.6338 - val_acc: 0.5391\n",
            "Epoch 5/100\n",
            "6/8 [=====================>........] - ETA: 25s - loss: 0.6030 - acc: 0.7318"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH9AxDTfp7fk",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating loss and accuracy of model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CTD24MSuEZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpkxLW5QsSJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}